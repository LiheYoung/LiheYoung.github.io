<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Lihe Yang</title>
  
  <meta name="author" content="Lihe Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/hku_icon.jpeg">

  <script type="text/javascript">

    function display(id) {
      var traget = document.getElementById(id);
      if (traget.style.display == "none") {
        traget.style.display = "";
      } else {
        traget.style.display = "none";
      }
    }  
  </script>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Lihe Yang</name>
              </p>
              <p> I am a second-year PhD student at 
                <a href="https://www.hku.hk/">The University of Hong Kong</a>, fortunately supervised by Prof. <a href="https://hszhao.github.io/">Hengshuang Zhao</a>. 
              </p>
              <p> I received my Bachelor and Master degree from <a href="https://www.nju.edu.cn/">Nanjing University</a> in 2020 and 2023 respectively, fortunately supervised by Prof. <a href="https://cs.nju.edu.cn/shiyh/index.htm">Yinghuan Shi</a>.
              </p>
              <p> My current research focuses on vision foundation model and vision-language model. Previously, I worked on learning from cheap visual data.
              </p>
              <p style="text-align:center">
                <a href="mailto:lihe.yang.cs@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=QX7xv3UAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/LiheYoung">GitHub</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:80%;max-width:80%" alt="profile photo" src="images/LiheYang.png" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p> <strong>2024-09:</strong> <a href="https://depth-anything-v2.github.io/">Depth Anything V2</a> is accepted by <em>NeurIPS</em> 2024 and integrated into <a href="https://developer.apple.com/machine-learning/models">Apple Core ML Models</a>. </p>
            <p> <strong>2024-02:</strong> <a href="https://depth-anything.github.io/">Depth Anything</a> is accepted by <em>CVPR</em> 2024. </p>
            <!-- <p> <strong>2023-12:</strong> I am awarded the <em>NeurIPS</em> 2023 Top Reviewer. </p> -->
            <p> <strong>2023-09:</strong> One paper on generative perception is accepted by <em>NeurIPS</em> 2023. </p>
            <p> <strong>2023-09:</strong> I start at HKU as a PhD student.</p>
            <p> <strong>2023-07:</strong> Two papers on semi-supervised learning and semantic segmentation are accepted by <em>ICCV</em> 2023. </p>
	          <p> <strong>2023-02:</strong> Two papers on semi-supervised semantic segmentation are accepted by <em>CVPR</em> 2023. </p>
            <p> <strong>2023-02:</strong> I am awarded the HKU Presidential PhD Scholarship. </p>
            <a onclick="return display('old_news');"> ---- show more ----</a>
            <div id="old_news" style="display: none;">
              <p> <strong>2022-03:</strong> One paper on semi-supervised semantic segmentation is accepted by <em>CVPR</em> 2022. </p>
              <p> <strong>2021-10:</strong> I am awarded the National Scholarship. </p>
              <p> <strong>2021-07:</strong> One paper on few-shot segmentation is accepted by <em>ICCV</em> 2021 as an Oral presentation. </p>
            </div>
            
          </td>
        </tr>
      
        
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Selected Publications</heading>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <video muted autoplay="autoplay" loop="loop" src="images/depth_anything_v2.mov" width="180" style="border-style: none"></video>
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2406.09414">
              <papertitle>Depth Anything V2</papertitle>
            </a>
            <br>
            <strong>Lihe Yang</strong>, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao
            <br>
            <em>Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</em>, 2024
            <br>
            <a href="https://depth-anything-v2.github.io">Page</a> / <a href="https://arxiv.org/abs/2406.09414">Paper</a> /
            <a href="https://github.com/DepthAnything/Depth-Anything-V2">Code</a> / <a href="https://huggingface.co/spaces/depth-anything/Depth-Anything-V2">Demo</a> / <a href="https://x.com/_akhaliq/status/1801432403665125738">Media</a>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <video muted autoplay="autoplay" loop="loop" src="images/depth_anything.mp4" width="180" style="border-style: none"></video>
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2401.10891">
              <papertitle>Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</papertitle>
            </a>
            <br>
            <strong>Lihe Yang</strong>, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao
            <br>
            <em>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2024
            <br>
            <a href="https://cvpr.thecvf.com/Conferences/2024/News/Wrap_Release">Best Demo Honorable Mention</a>
            <br>
            <a href="https://depth-anything.github.io">Page</a> / <a href="https://arxiv.org/abs/2401.10891">Paper</a> /
            <a href="https://github.com/LiheYoung/Depth-Anything">Code</a> / <a href="https://huggingface.co/spaces/LiheYoung/Depth-Anything">Demo</a> / <a href="https://twitter.com/_akhaliq/status/1749284669936275463">Media</a>
          </td>
        </tr>
        
        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/freemask.png' alt="game" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2310.15160">
              <papertitle>FreeMask: Synthetic Images with Dense Annotations Make Stronger Segmentation Models</papertitle>
            </a>
            <br>
            <strong>Lihe Yang</strong>, Xiaogang Xu, Bingyi Kang, Yinghuan Shi, Hengshuang Zhao
            <br>
            <em>Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</em>, 2023
            <br>
            <a href="https://arxiv.org/abs/2310.15160">Paper</a> /
            <a href="https://github.com/LiheYoung/FreeMask">Code</a> /
	    <a href="https://zhuanlan.zhihu.com/p/663587260">Zhihu</a>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/shrinkmatch.png' alt="game" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2308.06777">
              <papertitle>Shrinking Class Space for Enhanced Certainty in Semi-Supervised Learning</papertitle>
            </a>
            <br>
            <strong>Lihe Yang</strong>, Zhen Zhao, Lei Qi, Yu Qiao, Yinghuan Shi, Hengshuang Zhao
            <br>
            <em>IEEE International Conference on Computer Vision (<b>ICCV</b>)</em>, 2023
            <br>
            <a href="https://arxiv.org/abs/2308.06777">Paper</a> /
            <a href="https://github.com/LiheYoung/ShrinkMatch">Code</a> /
	    <a href="https://zhuanlan.zhihu.com/p/663191366">Zhihu</a>
          </td>
        </tr>

        <!-- <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/diverse_cotraining.png' alt="game" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2308.09281">
              <papertitle>Diverse Cotraining Makes Strong Semi-Supervised Segmentor</papertitle>
            </a>
            <br>
            Yijiang Li, Xinjiang Wang, <strong>Lihe Yang</strong>, Litong Feng, Wayne Zhang, Ying Gao
            <br>
            <em>IEEE International Conference on Computer Vision (<b>ICCV</b>)</em>, 2023
            <br>
            <a href="https://arxiv.org/abs/2308.09281">Paper</a> /
            <a href="https://github.com/williamium3000/diverse-cotraining">Code</a>
          </td>
        </tr> -->

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/unimatch.png' alt="game" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2208.09910">
              <papertitle>Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation</papertitle>
            </a>
            <br>
            <strong>Lihe Yang</strong>, Lei Qi, Litong Feng, Wayne Zhang, Yinghuan Shi
            <br>
            <em>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
            <br>
            <a href="https://arxiv.org/abs/2208.09910">Paper</a> /
            <a href="https://github.com/LiheYoung/UniMatch">Code</a> /
            <a href="https://zhuanlan.zhihu.com/p/617650677">Zhihu</a>
          </td>
        </tr>

        <!-- <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/augseg.png' alt="game" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2212.04976">
              <papertitle>Augmentation Matters: A Simple-yet-Effective Approach to Semi-supervised Semantic Segmentation</papertitle>
            </a>
            <br>
            Zhen Zhao, <strong>Lihe Yang</strong>, Sifan Long, Jimin Pi, Luping Zhou, Jingdong Wang
            <br>
            <em>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
            <br>
            <a href="https://arxiv.org/abs/2212.04976">Paper</a> /
            <a href="https://github.com/ZhenZHAO/AugSeg">Code</a>
          </td>
        </tr> -->

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/st++.png' alt="game" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2106.05095">
              <papertitle>ST++: Make Self-training Work Better for Semi-supervised Semantic Segmentation</papertitle>
            </a>
            <br>
            <strong>Lihe Yang</strong>, Wei Zhuo, Lei Qi, Yinghuan Shi, Yang Gao
            <br>
            <em>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
            <br>
            <a href="https://arxiv.org/abs/2106.05095">Paper</a> /
            <a href="https://github.com/LiheYoung/ST-PlusPlus">Code</a> /
            <a href="https://zhuanlan.zhihu.com/p/476692814">Zhihu</a>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/miningfss.png' alt="game" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2103.15402">
              <papertitle>Mining Latent Classes for Few-shot Segmentation</papertitle>
            </a>
            <br>
            <strong>Lihe Yang</strong>, Wei Zhuo, Lei Qi, Yinghuan Shi, Yang Gao
            <br>
            <em>IEEE International Conference on Computer Vision (<b>ICCV</b>)</em>, 2021 <b>(Oral)</b>
            <br>
            <a href="https://arxiv.org/abs/2103.15402">Paper</a> /
            <a href="https://github.com/LiheYoung/MiningFSS">Code</a>
          </td>
        </tr>

      </tbody></table>

      <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Talks</heading>
            <p>
              <b>Apple Research:</b> “Vision Foundation Model: Depth Anything", Apr. 2024
            </p>
            <p>
              <b>Shenzhen University:</b> “Learning from Cheap Visual Data", Apr. 2024
            </p>
            <p>
              <b>VALSE Webinar:</b> “Vision Foundation Model: Depth Anything", Mar. 2024
            </p>
          </td>
        </tr>
      </tbody></table> -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Contests</heading>
              <p>
                <b>1st Place</b>, SenseTime Remote Sensing Change Detection Competition, 30,000 RMB Bonus, 2020
              </p>
              <p>
                <b>2nd Place</b>, The 8th CCF BDCI Remote Sensing Image Segmentation, 20,000 RMB Bonus, 2020
              </p>
              <p>
                <b>2nd Place</b>, Off Road Image Segmentation Challenge, 200,000 Yen Bonus, 2020
              </p>
              <p>
                <b>2nd Place</b>, BAAI Ultra-high Resolution EM Images Segmentation, 10,000 RMB Bonus, 2019
              </p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Honors</heading>
              <p>
                <b>CVPR Best Demo Honorable Mention</b>, CVPR, 2024
              </p>

              <p>
                <b>NeurIPS Top Reviewer</b>, NeurIPS, 2023
              </p>
              <p>
                <b>HKU Presidential PhD Scholarship</b>, The University of Hong Kong, 2023-2027
              </p>
              <!-- <p>
                <b>Tencent Scholarship</b>, Nanjing University, 2022
              </p> -->
              <!-- <p>
                <b>First Prize Scholarship for Postgraduate Students</b>, Nanjing University, 2020-2022
              </p> -->
              <p>
              <b>National Scholarship</b>, Ministry of Education of P.R. China, 2021
              </p>
              <!-- <p>
                <b>First Prize of Excellent Undergraduate Thesis</b>, Nanjing University, 2020
              </p>
              <p>
                <b>MICCAI Undergraduate Student Travel Award</b>, MICCAI, 2019
              </p> -->
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Academic Services</heading>
              <p>
                Conference reviewer: CVPR, ICCV, ECCV, NeurIPS, ICLR, ICML, etc.
              </p>

              <p>
                Journal reviewer: TPAMI, TIP, TNNLS, TCSVT, TGRS, etc.
              </p>
              
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <br>
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
